import random
import torch
import gradio as gr
from PIL import Image
from diffsynth import save_video, VideoData
from diffsynth.pipelines.wan_video_new import WanVideoPipeline, ModelConfig
import time
import os
import gc
import tempfile
import shutil
import json
from datetime import datetime

from utils.video_utils import clean_temp_videos, reencode_video_to_16fps, get_video_info
from utils.img_utils import get_image_info
from utils.vram_utils import clear_vram, get_vram_info
from utils.animate.preprocess.process_pipepline import ProcessPipeline

ASPECT_RATIOS_14b = {
    "1:1":  (960, 960),
    "4:3":  (1104, 832),
    "3:4":  (832, 1104),
    "3:2":  (1152, 768),
    "2:3":  (768, 1152),
    "16:9": (1280, 720),
    "16:9_low": (832, 480),
    "9:16": (720, 1280),
    "9:16_low": (480, 832),
    "21:9": (1472, 624),
    "9:21": (624, 1472),
    "4:5":  (864, 1072),
    "5:4":  (1072, 864),
}

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# å…¨å±€å˜é‡å­˜å‚¨pipelineå’Œæ¨¡å‹é€‰æ‹©
pipe:WanVideoPipeline = None
selected_model = "PAI/Wan2.2-VACE-Fun-A14B"  # é»˜è®¤é€‰æ‹©14Bæ¨¡å‹
input_mode = "vace"  # é»˜è®¤è¾“å…¥æ¨¡å¼ï¼švaceï¼ˆæ·±åº¦è§†é¢‘+å‚è€ƒå›¾ç‰‡ï¼‰æˆ– inpï¼ˆé¦–å°¾å¸§ï¼‰

# å®šä¹‰ä¸åŒæ¨¡å¼å¯¹åº”çš„æ¨¡å‹åˆ—è¡¨
VACE_MODELS = [
    "PAI/Wan2.2-VACE-Fun-A14B",
    "Wan-AI/Wan2.1-VACE-14B",
    "Wan-AI/Wan2.1-VACE-1.3B"
]

INP_MODELS = [
    "PAI/Wan2.2-Fun-A14B-InP",
    "PAI/Wan2.1-Fun-14B-InP",
    "PAI/Wan2.1-Fun-V1.1-1.3B-InP"
]

ANIMATE_MODELS = [
    "Wan-AI/Wan2.2-Animate-14B"
]

def get_models_by_mode(mode):
    """æ ¹æ®è¾“å…¥æ¨¡å¼è¿”å›å¯¹åº”çš„æ¨¡å‹åˆ—è¡¨"""
    if mode == "vace":
        return VACE_MODELS
    elif mode == "inp":
        return INP_MODELS
    elif mode == "animate":
        return ANIMATE_MODELS
    else:
        return VACE_MODELS  # é»˜è®¤è¿”å›VACEæ¨¡å‹

def handle_tab_change(evt: gr.SelectData):
    """å¤„ç†Tabåˆ‡æ¢äº‹ä»¶"""
    # evt.index: 0 = VACEæ¨¡å¼, 1 = é¦–å°¾å¸§æ¨¡å¼, 2 = Animateæ¨¡å¼
    if evt.index == 0:  # VACEæ¨¡å¼
        models = VACE_MODELS
        default_model = VACE_MODELS[0]
        mode = "vace"
    elif evt.index == 1:  # é¦–å°¾å¸§æ¨¡å¼
        models = INP_MODELS
        default_model = INP_MODELS[0]
        mode = "inp"
    else:  # Animateæ¨¡å¼
        models = ANIMATE_MODELS
        default_model = ANIMATE_MODELS[0]
        mode = "animate"
    
    # æ›´æ–°å…¨å±€å˜é‡
    global input_mode, selected_model
    input_mode = mode
    selected_model = default_model
    
    # è¿”å›æ–°çš„æ¨¡å‹é€‰æ‹©åˆ—è¡¨å’Œé»˜è®¤å€¼
    return gr.Dropdown(choices=models, value=default_model)


def update_dimensions(aspect_ratio):
    """æ ¹æ®é€‰æ‹©çš„å®½é«˜æ¯”æ›´æ–°é«˜åº¦å’Œå®½åº¦"""
    if aspect_ratio in ASPECT_RATIOS_14b:
        width, height = ASPECT_RATIOS_14b[aspect_ratio]
        return height, width
    return 480, 832  # é»˜è®¤å€¼


def update_size_display(aspect_ratio):
    """æ›´æ–°Dropdownçš„infoæ˜¾ç¤ºå½“å‰å°ºå¯¸"""
    if aspect_ratio in ASPECT_RATIOS_14b:
        width, height = ASPECT_RATIOS_14b[aspect_ratio]
        size_text = f"{width} Ã— {height}"
    else:
        size_text = "832 Ã— 480"  # é»˜è®¤å€¼
    
    info_text = f"é€‰æ‹©é¢„è®¾çš„å®½é«˜æ¯”ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è®¡ç®—å¯¹åº”çš„å°ºå¯¸\nå½“å‰å°ºå¯¸: {size_text}"
    return gr.Dropdown(info=info_text)


def save_generation_results(output_video_path, save_folder_path, input_files, generation_params):
    """ä¿å­˜ç”Ÿæˆç»“æœï¼šåˆ›å»ºå­æ–‡ä»¶å¤¹ï¼Œå¤åˆ¶æ–‡ä»¶ï¼Œä¿å­˜å‚æ•°"""
    try:
        # åˆ›å»ºæ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        subfolder_name = f"generation_{timestamp}"
        subfolder_path = os.path.join(save_folder_path, subfolder_name)
        
        # åˆ›å»ºå­æ–‡ä»¶å¤¹
        os.makedirs(subfolder_path, exist_ok=True)
        print(f"åˆ›å»ºä¿å­˜æ–‡ä»¶å¤¹: {subfolder_path}")
        
        # å¤åˆ¶è¾“å‡ºè§†é¢‘
        if output_video_path and os.path.exists(output_video_path):
            video_filename = os.path.basename(output_video_path)
            new_video_path = os.path.join(subfolder_path, video_filename)
            shutil.copy2(output_video_path, new_video_path)
            print(f"å¤åˆ¶è¾“å‡ºè§†é¢‘: {new_video_path}")
        
        # å¤åˆ¶è¾“å…¥æ–‡ä»¶
        copied_files = {}
        for file_type, file_data in input_files.items():
            if file_data is not None:
                try:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯PIL Imageå¯¹è±¡
                    if hasattr(file_data, 'save'):
                        # æ˜¯PIL Imageå¯¹è±¡ï¼Œç›´æ¥ä¿å­˜
                        filename = f"{file_type}.png"
                        new_file_path = os.path.join(subfolder_path, filename)
                        file_data.save(new_file_path)
                        copied_files[file_type] = filename
                        print(f"ä¿å­˜{file_type}å›¾ç‰‡: {new_file_path}")
                    elif isinstance(file_data, str) and os.path.exists(file_data):
                        # æ˜¯æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²ï¼Œå¤åˆ¶æ–‡ä»¶
                        filename = os.path.basename(file_data)
                        new_file_path = os.path.join(subfolder_path, filename)
                        shutil.copy2(file_data, new_file_path)
                        copied_files[file_type] = filename
                        print(f"å¤åˆ¶{file_type}: {new_file_path}")
                    else:
                        print(f"è·³è¿‡{file_type}: æ— æ•ˆçš„æ–‡ä»¶æ•°æ®")
                except Exception as e:
                    print(f"å¤„ç†{file_type}æ—¶å‡ºé”™: {str(e)}")
                    continue
        
        # ä¿å­˜ç”Ÿæˆå‚æ•°åˆ°JSONæ–‡ä»¶
        params_data = {
            "generation_time": timestamp,
            "generation_params": generation_params,
            "input_files": copied_files,
            "output_video": os.path.basename(output_video_path) if output_video_path else None
        }
        
        params_file = os.path.join(subfolder_path, "generation_params.json")
        with open(params_file, 'w', encoding='utf-8') as f:
            json.dump(params_data, f, ensure_ascii=False, indent=2)
        print(f"ä¿å­˜å‚æ•°æ–‡ä»¶: {params_file}")
        
        return subfolder_path, f"ç”Ÿæˆç»“æœå·²ä¿å­˜åˆ°: {subfolder_path}"
        
    except Exception as e:
        error_msg = f"ä¿å­˜ç”Ÿæˆç»“æœæ—¶å‡ºé”™: {str(e)}"
        print(error_msg)
        return None, error_msg


def preprocess_template_video(template_video_path, reference_image_path, width, height, num_frames):
    """é¢„å¤„ç†æ¨¡æ¿è§†é¢‘ï¼Œç”Ÿæˆposeå’Œfaceè§†é¢‘"""
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨é¢„å¤„ç†ç»“æœ
        temp_dir = tempfile.mkdtemp(prefix="wanvace_preprocess_")
        
        # æ„å»ºæ¨¡å‹è·¯å¾„
        ckpt_path = "./models"
        pose2d_checkpoint_path = os.path.join(ckpt_path, 'pose2d/vitpose_h_wholebody.onnx')
        det_checkpoint_path = os.path.join(ckpt_path, 'det/yolov10m.onnx')
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(pose2d_checkpoint_path):
            error_msg = f"å§¿æ€æ£€æµ‹æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {pose2d_checkpoint_path}"
            print(error_msg)
            shutil.rmtree(temp_dir, ignore_errors=True)
            return None, None, error_msg
        
        if not os.path.exists(det_checkpoint_path):
            error_msg = f"ç›®æ ‡æ£€æµ‹æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {det_checkpoint_path}"
            print(error_msg)
            shutil.rmtree(temp_dir, ignore_errors=True)
            return None, None, error_msg
        
        print(f"å¼€å§‹é¢„å¤„ç†æ¨¡æ¿è§†é¢‘...")
        print(f"æ¨¡æ¿è§†é¢‘: {template_video_path}")
        print(f"å‚è€ƒå›¾ç‰‡: {reference_image_path}")
        print(f"ç›®æ ‡åˆ†è¾¨ç‡: {width}x{height}")
        print(f"è¾“å‡ºç›®å½•: {temp_dir}")
        
        # åˆ›å»ºProcessPipelineå®ä¾‹
        process_pipeline = ProcessPipeline(
            det_checkpoint_path=det_checkpoint_path,
            pose2d_checkpoint_path=pose2d_checkpoint_path,
            sam_checkpoint_path=None,  # ä¸ä½¿ç”¨SAM
            flux_kontext_path=None     # ä¸ä½¿ç”¨FLUX
        )
        
        # è°ƒç”¨é¢„å¤„ç†
        process_pipeline(
            video_path=template_video_path,
            refer_image_path=reference_image_path,
            output_path=temp_dir,
            resolution_area=[width, height],
            fps=30,  # ä½¿ç”¨é»˜è®¤FPS
            iterations=3,
            k=7,
            w_len=1,
            h_len=1,
            retarget_flag=True,  # å¯ç”¨å§¿æ€é‡å®šå‘
            use_flux=False,
            replace_flag=False
        )
        
        # æŸ¥æ‰¾ç”Ÿæˆçš„poseå’Œfaceè§†é¢‘
        pose_video_path = os.path.join(temp_dir, "src_pose.mp4")
        face_video_path = os.path.join(temp_dir, "src_face.mp4")
        
        if not os.path.exists(pose_video_path):
            error_msg = "é¢„å¤„ç†æœªç”Ÿæˆposeè§†é¢‘æ–‡ä»¶"
            print(error_msg)
            shutil.rmtree(temp_dir, ignore_errors=True)
            return None, None, error_msg
        
        if not os.path.exists(face_video_path):
            print("è­¦å‘Š: æœªæ‰¾åˆ°faceè§†é¢‘æ–‡ä»¶ï¼Œå°†åªä½¿ç”¨poseè§†é¢‘")
            face_video_path = None
        
        print(f"é¢„å¤„ç†æˆåŠŸå®Œæˆ")
        print(f"Poseè§†é¢‘: {pose_video_path}")
        print(f"Faceè§†é¢‘: {face_video_path}")
        
        return pose_video_path, face_video_path, temp_dir
        
    except Exception as e:
        error_msg = f"é¢„å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        print(error_msg)
        shutil.rmtree(temp_dir, ignore_errors=True)
        return None, None, error_msg


def initialize_pipeline(model_id="PAI/Wan2.2-VACE-Fun-A14B", vram_limit=6.0):
    """åˆå§‹åŒ–WanVideoPipeline"""
    global pipe, selected_model, input_mode
    
    # å¦‚æœæ¨¡å‹æ”¹å˜äº†ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–
    if pipe is not None and selected_model != model_id:
        print(f"æ¨¡å‹ä» {selected_model} åˆ‡æ¢åˆ° {model_id}ï¼Œé‡æ–°åˆå§‹åŒ–...")
        del pipe
        pipe = None
        torch.cuda.empty_cache()
        gc.collect()
    
    if pipe is None:
        print(f"æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {model_id}")
        selected_model = model_id
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®è¾“å…¥æ¨¡å¼
        if "InP" in model_id:
            input_mode = "inp"
        elif "Animate" in model_id:
            input_mode = "animate"
        else:
            input_mode = "vace"
        
        if model_id == "PAI/Wan2.2-VACE-Fun-A14B":
            # 14B VACEæ¨¡å‹é…ç½®
            pipe = WanVideoPipeline.from_pretrained(
                torch_dtype=torch.bfloat16,
                device="cuda",
                model_configs=[
                    ModelConfig(model_id="PAI/Wan2.2-VACE-Fun-A14B", origin_file_pattern="high_noise_model/diffusion_pytorch_model*.safetensors", offload_device="cpu"),
                    ModelConfig(model_id="PAI/Wan2.2-VACE-Fun-A14B", origin_file_pattern="low_noise_model/diffusion_pytorch_model*.safetensors", offload_device="cpu"),
                    ModelConfig(model_id="PAI/Wan2.2-VACE-Fun-A14B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
                    ModelConfig(model_id="PAI/Wan2.2-VACE-Fun-A14B", origin_file_pattern="Wan2.1_VAE.pth", offload_device="cpu"),
                ],
            )
        elif model_id == "Wan-AI/Wan2.1-VACE-1.3B":
            # 1.3B VACEæ¨¡å‹é…ç½®
            pipe = WanVideoPipeline.from_pretrained(
                torch_dtype=torch.bfloat16,
                device="cuda",
                model_configs=[
                    ModelConfig(model_id="Wan-AI/Wan2.1-VACE-1.3B", origin_file_pattern="diffusion_pytorch_model*.safetensors", offload_device="cpu"),
                    ModelConfig(model_id="Wan-AI/Wan2.1-VACE-1.3B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
                    ModelConfig(model_id="Wan-AI/Wan2.1-VACE-1.3B", origin_file_pattern="Wan2.1_VAE.pth", offload_device="cpu"),
                ],
            )
        elif model_id == "PAI/Wan2.2-Fun-A14B-InP":
            # 14B InPæ¨¡å‹é…ç½®
            pipe = WanVideoPipeline.from_pretrained(
                torch_dtype=torch.bfloat16,
                device="cuda",
                model_configs=[
                    ModelConfig(model_id="PAI/Wan2.2-Fun-A14B-InP", origin_file_pattern="high_noise_model/diffusion_pytorch_model*.safetensors", offload_device="cpu"),
                    ModelConfig(model_id="PAI/Wan2.2-Fun-A14B-InP", origin_file_pattern="low_noise_model/diffusion_pytorch_model*.safetensors", offload_device="cpu"),
                    ModelConfig(model_id="PAI/Wan2.2-Fun-A14B-InP", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
                    ModelConfig(model_id="PAI/Wan2.2-Fun-A14B-InP", origin_file_pattern="Wan2.1_VAE.pth", offload_device="cpu"),
                ],
            )
        elif model_id == "PAI/Wan2.1-Fun-V1.1-1.3B-InP":
            # 1.3B InPæ¨¡å‹é…ç½®
            pipe = WanVideoPipeline.from_pretrained(
                torch_dtype=torch.bfloat16,
                device="cuda",
                model_configs=[
                    ModelConfig(model_id="PAI/Wan2.1-Fun-V1.1-1.3B-InP", origin_file_pattern="diffusion_pytorch_model*.safetensors", offload_device="cpu"),
                    ModelConfig(model_id="PAI/Wan2.1-Fun-V1.1-1.3B-InP", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
                    ModelConfig(model_id="PAI/Wan2.1-Fun-V1.1-1.3B-InP", origin_file_pattern="Wan2.1_VAE.pth", offload_device="cpu"),
                    ModelConfig(model_id="PAI/Wan2.1-Fun-1.3B-InP", origin_file_pattern="models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth", offload_device="cpu"),
                ],
            )
        elif model_id == "Wan-AI/Wan2.2-Animate-14B":
            # 14B Animateæ¨¡å‹é…ç½®ï¼ˆä¸test.pyä¿æŒä¸€è‡´ï¼‰
            pipe = WanVideoPipeline.from_pretrained(
                torch_dtype=torch.bfloat16,
                device="cuda",
                model_configs=[
                    ModelConfig(model_id="Wan-AI/Wan2.2-Animate-14B", origin_file_pattern="diffusion_pytorch_model*.safetensors", offload_device="cpu"),
                    ModelConfig(model_id="Wan-AI/Wan2.2-Animate-14B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
                    ModelConfig(model_id="Wan-AI/Wan2.2-Animate-14B", origin_file_pattern="Wan2.1_VAE.pth", offload_device="cpu"),
                    ModelConfig(model_id="Wan-AI/Wan2.2-Animate-14B", origin_file_pattern="models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth", offload_device="cpu"),
                ],
            )
        
        # å°†GBè½¬æ¢ä¸ºå­—èŠ‚
        num_persistent_param_in_dit = int(vram_limit * 1024**3)
        pipe.enable_vram_management(num_persistent_param_in_dit=num_persistent_param_in_dit)
        
        print(f"Pipelineåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {model_id}")
        print(f"è¾“å…¥æ¨¡å¼: {input_mode}")
        print(f"æ˜¾å­˜é™åˆ¶: {vram_limit}GB")
    return f"Pipelineåˆå§‹åŒ–å®Œæˆï¼ä½¿ç”¨æ¨¡å‹: {model_id} (æ¨¡å¼: {input_mode})"


def process_video(
    depth_video,
    reference_image,
    prompt,
    negative_prompt,
    seed,
    fps,
    quality,
    height,
    width,
    num_frames,
    num_inference_steps,
    vram_limit,
    model_id,
    first_frame=None,
    last_frame=None,
    tiled=True,
    animate_reference_image=None,
    template_video=None,
    save_folder_path="./outputs"
):
    """å¤„ç†è§†é¢‘ç”Ÿæˆ"""
    try:
        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ¤æ–­è¾“å…¥æ¨¡å¼
        is_inp_mode = "InP" in model_id
        is_animate_mode = "Animate" in model_id
        
        if is_inp_mode:
            # InPæ¨¡å¼ï¼šéœ€è¦é¦–å¸§ï¼Œå°¾å¸§å¯é€‰
            has_first_frame = first_frame is not None
            if not has_first_frame:
                return None, "é”™è¯¯ï¼šé¦–å°¾å¸§æ¨¡å¼éœ€è¦ä¸Šä¼ é¦–å¸§å›¾ç‰‡"
        elif is_animate_mode:
            # Animateæ¨¡å¼ï¼šéœ€è¦å‚è€ƒå›¾ç‰‡å’Œæ¨¡æ¿è§†é¢‘
            has_reference_image = animate_reference_image is not None
            has_template_video = template_video is not None and template_video != ""
            if not has_reference_image:
                return None, "é”™è¯¯ï¼šAnimateæ¨¡å¼éœ€è¦ä¸Šä¼ å‚è€ƒå›¾ç‰‡"
            if not has_template_video:
                return None, "é”™è¯¯ï¼šAnimateæ¨¡å¼éœ€è¦ä¸Šä¼ æ¨¡æ¿è§†é¢‘"
        else:
            # VACEæ¨¡å¼ï¼šéœ€è¦æ·±åº¦è§†é¢‘æˆ–å‚è€ƒå›¾ç‰‡
            has_depth_video = depth_video is not None and depth_video != ""
            has_reference_image = reference_image is not None
            if not has_depth_video and not has_reference_image:
                return None, "é”™è¯¯ï¼šè¯·è‡³å°‘ä¸Šä¼ æ·±åº¦è§†é¢‘æˆ–å‚è€ƒå›¾ç‰‡ä¸­çš„ä¸€ç§"
        
        if seed < 0:
            seed = random.randint(1, 2**32 - 1)
        
        # è‡ªåŠ¨åˆå§‹åŒ–pipelineï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
        if pipe is None:
            status = initialize_pipeline(model_id, vram_limit)
            if "å®Œæˆ" not in status:
                return None, f"model_idåˆå§‹åŒ–å¤±è´¥ï¼š{status}"
        
        vace_video = None
        vace_reference_image = None
        animate_reference_img = None
        template_video_data = None

        if not prompt:
            prompt = "ä¸¤åªå¯çˆ±çš„æ©˜çŒ«æˆ´ä¸Šæ‹³å‡»æ‰‹å¥—ï¼Œç«™åœ¨ä¸€ä¸ªæ‹³å‡»å°ä¸Šææ–—ã€‚"
        
        if not negative_prompt:
            negative_prompt = "è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
        
        if is_inp_mode:
            # InPæ¨¡å¼ï¼šå¤„ç†é¦–å°¾å¸§
            if has_first_frame:
                print(f"è°ƒè¯•ä¿¡æ¯ï¼šé¦–å¸§å›¾ç‰‡ = {first_frame}, ç±»å‹ = {type(first_frame)}")
                if isinstance(first_frame, str):
                    first_frame_img = Image.open(first_frame).resize((width, height)).convert("RGB")
                else:
                    first_frame_img = first_frame.resize((width, height)).convert("RGB")
                
                # å¦‚æœæœ‰å°¾å¸§ï¼Œä¹Ÿå¤„ç†
                last_frame_img = None
                if last_frame is not None:
                    print(f"è°ƒè¯•ä¿¡æ¯ï¼šå°¾å¸§å›¾ç‰‡ = {last_frame}, ç±»å‹ = {type(last_frame)}")
                    if isinstance(last_frame, str):
                        last_frame_img = Image.open(last_frame).resize((width, height)).convert("RGB")
                    else:
                        last_frame_img = last_frame.resize((width, height)).convert("RGB")
                
                # è°ƒç”¨InPæ¨¡å‹çš„pipeline
                video = pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    input_image=first_frame_img,
                    end_image=last_frame_img,
                    seed=seed,
                    tiled=True,
                    height=height,
                    width=width,
                    num_frames=num_frames,
                    num_inference_steps=num_inference_steps,
                    cfg_scale=6.0,
                    sigma_shift=6.0,
                )
            else:
                return None, "é¦–å°¾å¸§æ¨¡å¼éœ€è¦ä¸Šä¼ é¦–å¸§å›¾ç‰‡"
        elif is_animate_mode:
            # Animateæ¨¡å¼ï¼šå¤„ç†å‚è€ƒå›¾ç‰‡å’Œæ¨¡æ¿è§†é¢‘
            if has_reference_image:
                print(f"è°ƒè¯•ä¿¡æ¯ï¼šAnimateå‚è€ƒå›¾ç‰‡ = {animate_reference_image}, ç±»å‹ = {type(animate_reference_image)}")
                if isinstance(animate_reference_image, str):
                    animate_reference_img = Image.open(animate_reference_image).resize((width, height)).convert("RGB")
                else:
                    animate_reference_img = animate_reference_image.resize((width, height)).convert("RGB")
            
            if has_template_video:
                # é¢„å¤„ç†æ¨¡æ¿è§†é¢‘
                temp_template_path = template_video
                print(f"è°ƒè¯•ä¿¡æ¯ï¼šæ¨¡æ¿è§†é¢‘è·¯å¾„ = {temp_template_path}, ç±»å‹ = {type(temp_template_path)}")
                print(f"ç›®æ ‡å°ºå¯¸: {width}x{height}")
                
                # ä¿å­˜å‚è€ƒå›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶ç”¨äºé¢„å¤„ç†
                temp_ref_path = None
                if isinstance(animate_reference_image, str):
                    temp_ref_path = animate_reference_image
                else:
                    temp_ref_path = tempfile.mktemp(suffix=".png")
                    animate_reference_image.save(temp_ref_path)
                
                # è°ƒç”¨é¢„å¤„ç†å‡½æ•°
                pose_video_path, face_video_path, temp_dir = preprocess_template_video(
                    temp_template_path, temp_ref_path, width, height, num_frames
                )
                
                if pose_video_path is None:
                    return None, f"æ¨¡æ¿è§†é¢‘é¢„å¤„ç†å¤±è´¥ï¼š{face_video_path}"  # face_video_pathæ­¤æ—¶åŒ…å«é”™è¯¯ä¿¡æ¯
                
                try:
                    # åŠ è½½é¢„å¤„ç†åçš„poseè§†é¢‘
                    pose_video_data = VideoData(pose_video_path).raw_data()[:num_frames-4]
                    print(f"æˆåŠŸåŠ è½½poseè§†é¢‘æ•°æ®ï¼Œå¸§æ•°: {len(pose_video_data)}")
                    
                    # å¦‚æœæœ‰faceè§†é¢‘ï¼Œä¹ŸåŠ è½½
                    face_video_data = None
                    if face_video_path and os.path.exists(face_video_path):
                        face_video_data = VideoData(face_video_path).raw_data()[:num_frames-4]
                        print(f"æˆåŠŸåŠ è½½faceè§†é¢‘æ•°æ®ï¼Œå¸§æ•°: {len(face_video_data)}")
                    
                except Exception as e:
                    print(f"é¢„å¤„ç†è§†é¢‘åŠ è½½å¤±è´¥: {e}")
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    return None, f"é¢„å¤„ç†è§†é¢‘åŠ è½½å¤±è´¥ï¼š{str(e)}"
            
            # è°ƒç”¨Animateæ¨¡å‹çš„pipelineï¼ˆä¸test.pyä¿æŒä¸€è‡´ï¼‰
            video = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                input_image=animate_reference_img,
                animate_pose_video=pose_video_data,
                animate_face_video=face_video_data,
                seed=seed,
                tiled=True,
                height=height,
                width=width,
                num_frames=num_frames,
                num_inference_steps=num_inference_steps,
                cfg_scale=6.0,
                sigma_shift=6.0,
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'temp_dir' in locals():
                shutil.rmtree(temp_dir, ignore_errors=True)
        else:
            # VACEæ¨¡å¼ï¼šå¤„ç†æ·±åº¦è§†é¢‘å’Œå‚è€ƒå›¾ç‰‡
            if has_depth_video:
                # Gradio Videoç»„ä»¶è¿”å›çš„æ˜¯æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
                temp_video_path = depth_video
                print(f"è°ƒè¯•ä¿¡æ¯ï¼šæ·±åº¦è§†é¢‘è·¯å¾„ = {temp_video_path}, ç±»å‹ = {type(temp_video_path)}")
                print(f"ç›®æ ‡å°ºå¯¸: {width}x{height}")
                # æŠŠç›®æ ‡è§†é¢‘å®½é«˜å’Œå¸§ç‡è½¬æ¢ä¸º16fps
                temp_video_path = reencode_video_to_16fps(temp_video_path, num_frames, width, height)
                
                # åˆ›å»ºVideoDataæ—¶æŒ‡å®šç›®æ ‡å°ºå¯¸ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨è°ƒæ•´
                try:
                    vace_video = VideoData(temp_video_path, height=height, width=width)
                    print(f"æˆåŠŸåˆ›å»ºVideoDataï¼Œå°ºå¯¸: {width}x{height}")
                except Exception as e:
                    print(f"VideoDataåˆ›å»ºå¤±è´¥: {e}")
                    return None, f"æ·±åº¦è§†é¢‘å¤„ç†å¤±è´¥ï¼š{str(e)}\nè¯·ç¡®ä¿è§†é¢‘å°ºå¯¸ä¸ç›®æ ‡å°ºå¯¸å…¼å®¹"
            
            if has_reference_image:
                print(f"è°ƒè¯•ä¿¡æ¯ï¼šå‚è€ƒå›¾ç‰‡ = {reference_image}, ç±»å‹ = {type(reference_image)}")
                if isinstance(reference_image, str):
                    vace_reference_image = Image.open(reference_image).resize((width, height)).convert("RGB")
                else:
                    vace_reference_image = reference_image.resize((width, height)).convert("RGB")
        
            video = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                vace_video=vace_video,
                vace_reference_image=vace_reference_image,
                seed=seed,
                tiled=tiled,
                height=height,
                width=width,
                num_frames=num_frames,
                num_inference_steps=num_inference_steps,
                cfg_scale=6.0,
                sigma_shift=6.0,
                vace_scale=0.8,
            )
        
        timestamp = int(time.time())
        output_path = f"output_video_{seed}_{timestamp}.mp4"
        save_video(video, output_path, fps=fps, quality=quality)

        print("cleaning temp videos...")
        clean_temp_videos()
        print("clearing vram...")
        clear_vram(pipe)
        
        # å‡†å¤‡ä¿å­˜ç”Ÿæˆç»“æœ
        input_files = {}
        generation_params = {
            "prompt": prompt,
            "negative_prompt": negative_prompt,
            "seed": seed,
            "fps": fps,
            "quality": quality,
            "height": height,
            "width": width,
            "num_frames": num_frames,
            "num_inference_steps": num_inference_steps,
            "vram_limit": vram_limit,
            "model_id": model_id,
            "tiled": tiled,
            "input_mode": input_mode
        }
        
        # æ”¶é›†è¾“å…¥æ–‡ä»¶è·¯å¾„
        if depth_video:
            input_files["depth_video"] = depth_video
        if reference_image:
            input_files["reference_image"] = reference_image
        if first_frame:
            input_files["first_frame"] = first_frame
        if last_frame:
            input_files["last_frame"] = last_frame
        if animate_reference_image:
            input_files["animate_reference_image"] = animate_reference_image
        if template_video:
            input_files["template_video"] = template_video
        
        # ä¿å­˜ç”Ÿæˆç»“æœ
        if save_folder_path and save_folder_path.strip():
            # å¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
            if not os.path.isabs(save_folder_path):
                save_folder_path = os.path.abspath(save_folder_path)
            
            # ç¡®ä¿ä¿å­˜æ–‡ä»¶å¤¹å­˜åœ¨
            os.makedirs(save_folder_path, exist_ok=True)
            
            save_result, save_message = save_generation_results(
                output_path, save_folder_path, input_files, generation_params
            )
            
            if save_result:
                return output_path, f"è§†é¢‘ç”ŸæˆæˆåŠŸï¼å·²ä¿å­˜ä¸º {output_path}\n{save_message}"
            else:
                return output_path, f"è§†é¢‘ç”ŸæˆæˆåŠŸï¼å·²ä¿å­˜ä¸º {output_path}\nè­¦å‘Šï¼š{save_message}"
        else:
            return output_path, f"è§†é¢‘ç”ŸæˆæˆåŠŸï¼å·²ä¿å­˜ä¸º {output_path}"
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        return None, f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}\n\nè¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š\n{error_trace}"

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    with gr.Blocks(title="WanVACE è§†é¢‘ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¬ WanVACE è§†é¢‘ç”Ÿæˆå™¨")
        gr.Markdown("ä½¿ç”¨Wan2.1-VACE-14Bæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡è§†é¢‘")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“¤ è¾“å…¥è®¾ç½®")
                
                with gr.Tabs() as input_tabs:
                    with gr.TabItem("ğŸ¬ VACEæ¨¡å¼ (æ·±åº¦è§†é¢‘+å‚è€ƒå›¾ç‰‡)", id="vace_tab"):
                        depth_video = gr.Video(
                            label="æ·±åº¦è§†é¢‘ (Depth Video)",
                            height=200,
                        )
                        
                        video_info = gr.Textbox(
                            label="è§†é¢‘ä¿¡æ¯",
                            value="æœªä¸Šä¼ è§†é¢‘",
                            interactive=False,
                            info="æ˜¾ç¤ºä¸Šä¼ è§†é¢‘çš„åŸå§‹ä¿¡æ¯"
                        )
                        
                        reference_image = gr.Image(
                            label="å‚è€ƒå›¾ç‰‡ (Reference Image)",
                            height=200,
                            type="pil",
                        )
                        
                        reference_image_info = gr.Textbox(
                            label="å‚è€ƒå›¾ç‰‡ä¿¡æ¯",
                            value="æœªä¸Šä¼ å›¾ç‰‡",
                            interactive=False,
                            info="æ˜¾ç¤ºå‚è€ƒå›¾ç‰‡çš„å°ºå¯¸ã€æ ¼å¼ç­‰ä¿¡æ¯"
                        )
                    
                    with gr.TabItem("ğŸ–¼ï¸ é¦–å°¾å¸§æ¨¡å¼ (é¦–å¸§+å°¾å¸§)", id="inp_tab"):
                        first_frame = gr.Image(
                            label="é¦–å¸§å›¾ç‰‡ (First Frame)",
                            height=200,
                            type="pil",
                        )
                        
                        first_frame_info = gr.Textbox(
                            label="é¦–å¸§å›¾ç‰‡ä¿¡æ¯",
                            value="æœªä¸Šä¼ å›¾ç‰‡",
                            interactive=False,
                            info="æ˜¾ç¤ºé¦–å¸§å›¾ç‰‡çš„å°ºå¯¸ã€æ ¼å¼ç­‰ä¿¡æ¯"
                        )
                        
                        last_frame = gr.Image(
                            label="å°¾å¸§å›¾ç‰‡ (Last Frame)",
                            height=200,
                            type="pil"
                        )
                        
                        last_frame_info = gr.Textbox(
                            label="å°¾å¸§å›¾ç‰‡ä¿¡æ¯",
                            value="æœªä¸Šä¼ å›¾ç‰‡",
                            interactive=False,
                            info="æ˜¾ç¤ºå°¾å¸§å›¾ç‰‡çš„å°ºå¯¸ã€æ ¼å¼ç­‰ä¿¡æ¯"
                        )
                    
                    with gr.TabItem("ğŸ­ Animateæ¨¡å¼ (å‚è€ƒå›¾ç‰‡+æ¨¡æ¿è§†é¢‘)", id="animate_tab"):
                        animate_reference_image = gr.Image(
                            label="å‚è€ƒå›¾ç‰‡ (Reference Image)",
                            height=200,
                            type="pil",
                        )
                        
                        animate_reference_image_info = gr.Textbox(
                            label="å‚è€ƒå›¾ç‰‡ä¿¡æ¯",
                            value="æœªä¸Šä¼ å›¾ç‰‡",
                            interactive=False,
                            info="æ˜¾ç¤ºå‚è€ƒå›¾ç‰‡çš„å°ºå¯¸ã€æ ¼å¼ç­‰ä¿¡æ¯"
                        )
                        
                        template_video = gr.Video(
                            label="æ¨¡æ¿è§†é¢‘ (Template Video)",
                            height=200,
                        )
                        
                        template_video_info = gr.Textbox(
                            label="æ¨¡æ¿è§†é¢‘ä¿¡æ¯",
                            value="æœªä¸Šä¼ è§†é¢‘",
                            interactive=False,
                            info="æ˜¾ç¤ºæ¨¡æ¿è§†é¢‘çš„åŸå§‹ä¿¡æ¯"
                        )
                

                
            with gr.Column(scale=1):
                gr.Markdown("## âš™ï¸ å‚æ•°è®¾ç½®")
                
                # æ¨¡å‹é€‰æ‹©
                model_id = gr.Dropdown(
                    label="é€‰æ‹©æ¨¡å‹",
                    choices=VACE_MODELS,
                    value="PAI/Wan2.2-VACE-Fun-A14B",
                    info="æ¨¡å‹ä¼šæ ¹æ®é€‰æ‹©çš„è¾“å…¥æ¨¡å¼è‡ªåŠ¨æ›´æ–°"
                )
                
                prompt = gr.Textbox(
                    label="æ­£é¢æç¤ºè¯",
                    placeholder="ä¸¤åªå¯çˆ±çš„æ©˜çŒ«æˆ´ä¸Šæ‹³å‡»æ‰‹å¥—ï¼Œç«™åœ¨ä¸€ä¸ªæ‹³å‡»å°ä¸Šææ–—ã€‚",
                    lines=3
                )
                
                negative_prompt = gr.Textbox(
                    label="è´Ÿé¢æç¤ºè¯",
                    placeholder="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…...",
                    lines=4
                )
                
                with gr.Row():
                    seed = gr.Number(
                        label="éšæœºç§å­",
                        value=-1,
                        minimum=-1,
                    )
                    fps = gr.Slider(
                        label="FPS",
                        value=16,
                        minimum=1,
                        maximum=60,
                        step=1
                    )
                
                # è§†é¢‘å°ºå¯¸è®¾ç½®
                gr.Markdown("### ğŸ“ è§†é¢‘å°ºå¯¸è®¾ç½®")
                with gr.Tabs() as size_tabs:
                    with gr.TabItem("ğŸ“ é¢„è®¾å®½é«˜æ¯”", id="aspect_ratio_tab"):
                        aspect_ratio = gr.Dropdown(
                            label="é€‰æ‹©å®½é«˜æ¯”",
                            choices=list(ASPECT_RATIOS_14b.keys()),
                            value="16:9_low",
                            info="é€‰æ‹©é¢„è®¾çš„å®½é«˜æ¯”ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è®¡ç®—å¯¹åº”çš„å°ºå¯¸\nå½“å‰å°ºå¯¸: 832 Ã— 480"
                        )
                    
                    with gr.TabItem("ğŸ”§ æ‰‹åŠ¨è®¾ç½®", id="manual_size_tab"):
                        with gr.Row():
                            width = gr.Number(
                                label="è§†é¢‘å®½åº¦",
                                value=832,
                                minimum=256,
                                maximum=1280,
                                step=64,
                                info="è§†é¢‘å®½åº¦ï¼ˆåƒç´ ï¼‰"
                            )
                            height = gr.Number(
                                label="è§†é¢‘é«˜åº¦",
                                value=480,
                                minimum=256,
                                maximum=1280,
                                step=64,
                                info="è§†é¢‘é«˜åº¦ï¼ˆåƒç´ ï¼‰"
                            )
                
                # æ˜¾å­˜ç®¡ç†
                with gr.Row():
                    clear_vram_btn = gr.Button("ğŸ§¹ é‡Šæ”¾æ˜¾å­˜", variant="secondary", size="sm")
                    refresh_vram_btn = gr.Button("ğŸ”„ åˆ·æ–°æ˜¾å­˜ä¿¡æ¯", variant="secondary", size="sm")
                
                vram_info = gr.Textbox(
                    label="æ˜¾å­˜çŠ¶æ€",
                    value="ç‚¹å‡»åˆ·æ–°æŒ‰é’®æŸ¥çœ‹æ˜¾å­˜ä¿¡æ¯",
                    interactive=False,
                    info="æ˜¾ç¤ºå½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"
                )
                
                quality = gr.Slider(
                    label="è´¨é‡",
                    value=8,
                    minimum=1,
                    maximum=10,
                    step=1,
                    info="1=æœ€ä½è´¨é‡ï¼Œ10=æœ€é«˜è´¨é‡"
                )
                
                # æ–°å¢çš„é«˜çº§å‚æ•°è®¾ç½®
                gr.Markdown("### ğŸ”§ é«˜çº§å‚æ•°è®¾ç½®")
                
                with gr.Row():
                    num_frames = gr.Number(
                        label="è§†é¢‘å¸§æ•°",
                        value=81,
                        minimum=16,
                        maximum=256,
                        step=1,
                        info="è§†é¢‘æ€»å¸§æ•°ï¼Œå»ºè®®16-256ä¹‹é—´"
                    )
                    num_inference_steps = gr.Number(
                        label="æ¨ç†æ­¥æ•°",
                        value=40,
                        minimum=10,
                        maximum=100,
                        step=1,
                        info="æ¨ç†æ­¥æ•°ï¼Œæ­¥æ•°è¶Šå¤šè´¨é‡è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢"
                    )
                
                with gr.Row():
                    vram_limit = gr.Slider(
                        label="æ˜¾å­˜å ç”¨é‡é™åˆ¶",
                        value=6.0,
                        minimum=0.0,
                        maximum=100.0,
                        step=1,
                        info="æ˜¾å­˜å ç”¨é‡é™åˆ¶ï¼ˆGBï¼‰ï¼Œå½±å“æ˜¾å­˜ä½¿ç”¨å’Œæ€§èƒ½"
                    )
                    tiled_checkbox = gr.Checkbox(
                        label="Tiled VAE Decode", 
                        value=True, 
                        info="ç¦ç”¨å¯èƒ½å¯¼è‡´VAEé”™è¯¯ï¼Œä½†å¯æé«˜æ€§èƒ½"
                    )
                
                # è§†é¢‘ä¿å­˜è®¾ç½®
                gr.Markdown("### ğŸ’¾ è§†é¢‘ä¿å­˜è®¾ç½®")
                save_folder_path = gr.Textbox(
                    label="è§†é¢‘ä¿å­˜åœ°å€",
                    value="./outputs",
                    placeholder="./outputs æˆ– /path/to/save/folder",
                    info="æ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼Œæ¯æ¬¡ç”Ÿæˆä¼šåˆ›å»ºæ—¶é—´æˆ³å­æ–‡ä»¶å¤¹"
                )
                
                generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
        
        with gr.Row():
            gr.Markdown("## ğŸ“¹ è¾“å‡ºç»“æœ")
        
        with gr.Row():
            output_video = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")
            output_status = gr.Textbox(label="ç”ŸæˆçŠ¶æ€", interactive=False)
        
        # Tabåˆ‡æ¢æ—¶æ›´æ–°æ¨¡å‹é€‰æ‹©
        input_tabs.select(
            fn=handle_tab_change,
            outputs=[model_id]
        )
        
        # å®½é«˜æ¯”é€‰æ‹©å˜åŒ–æ—¶è‡ªåŠ¨æ›´æ–°å°ºå¯¸å’Œæ˜¾ç¤º
        aspect_ratio.change(
            fn=update_dimensions,
            inputs=[aspect_ratio],
            outputs=[height, width]
        )
        
        # å®½é«˜æ¯”é€‰æ‹©å˜åŒ–æ—¶æ›´æ–°Dropdownçš„infoæ˜¾ç¤º
        aspect_ratio.change(
            fn=update_size_display,
            inputs=[aspect_ratio],
            outputs=[aspect_ratio]
        )
        
        # è§†é¢‘ä¸Šä¼ æ—¶è‡ªåŠ¨æ›´æ–°è§†é¢‘ä¿¡æ¯
        depth_video.change(
            fn=get_video_info,
            inputs=[depth_video],
            outputs=[video_info]
        )
        
        # å›¾ç‰‡ä¸Šä¼ æ—¶è‡ªåŠ¨æ›´æ–°å›¾ç‰‡ä¿¡æ¯
        reference_image.change(
            fn=get_image_info,
            inputs=[reference_image],
            outputs=[reference_image_info]
        )
        
        first_frame.change(
            fn=get_image_info,
            inputs=[first_frame],
            outputs=[first_frame_info]
        )
        
        last_frame.change(
            fn=get_image_info,
            inputs=[last_frame],
            outputs=[last_frame_info]
        )
        
        # Animateæ¨¡å¼çš„äº‹ä»¶å¤„ç†
        animate_reference_image.change(
            fn=get_image_info,
            inputs=[animate_reference_image],
            outputs=[animate_reference_image_info]
        )
        
        template_video.change(
            fn=get_video_info,
            inputs=[template_video],
            outputs=[template_video_info]
        )
        
        # æ˜¾å­˜ç®¡ç†æŒ‰é’®äº‹ä»¶
        clear_vram_btn.click(
            fn=clear_vram,
            outputs=[vram_info]
        )
        
        refresh_vram_btn.click(
            fn=get_vram_info,
            outputs=[vram_info]
        )
        
        generate_btn.click(
            fn=process_video,
            inputs=[
                depth_video,
                reference_image,
                prompt,
                negative_prompt,
                seed,
                fps,
                quality,
                height,
                width,
                num_frames,
                num_inference_steps,
                vram_limit,
                model_id,
                first_frame,
                last_frame,
                tiled_checkbox,
                animate_reference_image,
                template_video,
                save_folder_path
            ],
            outputs=[output_video, output_status]
        )
        
        gr.Markdown("## ğŸ“š ä½¿ç”¨è¯´æ˜")
        gr.Markdown("""
        1. **é€‰æ‹©è¾“å…¥æ¨¡å¼**ï¼šç‚¹å‡»"VACEæ¨¡å¼"ã€"é¦–å°¾å¸§æ¨¡å¼"æˆ–"Animateæ¨¡å¼"æ ‡ç­¾é¡µ
        2. **é€‰æ‹©æ¨¡å‹**ï¼šæ¨¡å‹ä¼šæ ¹æ®é€‰æ‹©çš„æ ‡ç­¾é¡µè‡ªåŠ¨æ›´æ–°ï¼Œé€‰æ‹©é€‚åˆçš„æ¨¡å‹ï¼ˆ14Bè´¨é‡æ›´é«˜ï¼Œ1.3Bé€Ÿåº¦æ›´å¿«ï¼‰
        3. **ä¸Šä¼ æ–‡ä»¶**ï¼šæ ¹æ®é€‰æ‹©çš„æ¨¡å¼ä¸Šä¼ ç›¸åº”çš„æ–‡ä»¶
        4. **è®¾ç½®å‚æ•°**ï¼šè°ƒæ•´æç¤ºè¯ã€ç§å­ã€FPSã€è´¨é‡ã€è§†é¢‘å°ºå¯¸å’Œé«˜çº§å‚æ•°
        5. **è®¾ç½®ä¿å­˜åœ°å€**ï¼šæŒ‡å®šè§†é¢‘ä¿å­˜æ–‡ä»¶å¤¹ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼‰
        6. **ç”Ÿæˆè§†é¢‘**ï¼šç‚¹å‡»"ç”Ÿæˆè§†é¢‘"æŒ‰é’®å¼€å§‹å¤„ç†ï¼ˆé¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨åˆå§‹åŒ–æ¨¡å‹ï¼‰
        
        **æ ‡ç­¾é¡µä¸æ¨¡å‹å¯¹åº”å…³ç³»**ï¼š
        - **ğŸ¬ VACEæ¨¡å¼æ ‡ç­¾é¡µ**ï¼šæ˜¾ç¤ºVACEæ¨¡å‹
          - **PAI/Wan2.2-VACE-Fun-A14B**ï¼šé«˜è´¨é‡VACEæ¨¡å‹ï¼Œç”Ÿæˆæ•ˆæœæ›´å¥½ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜å’Œè®¡ç®—æ—¶é—´
          - **Wan-AI/Wan2.1-VACE-1.3B**ï¼šè½»é‡çº§VACEæ¨¡å‹ï¼Œç”Ÿæˆé€Ÿåº¦æ›´å¿«ï¼Œæ˜¾å­˜éœ€æ±‚æ›´å°‘ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•
        - **ğŸ–¼ï¸ é¦–å°¾å¸§æ¨¡å¼æ ‡ç­¾é¡µ**ï¼šæ˜¾ç¤ºInPæ¨¡å‹
          - **PAI/Wan2.2-Fun-A14B-InP**ï¼šé«˜è´¨é‡é¦–å°¾å¸§æ¨¡å‹ï¼Œ14Bå‚æ•°
          - **PAI/Wan2.1-Fun-V1.1-1.3B-InP**ï¼šè½»é‡çº§é¦–å°¾å¸§æ¨¡å‹ï¼Œ1.3Bå‚æ•°
        - **ğŸ­ Animateæ¨¡å¼æ ‡ç­¾é¡µ**ï¼šæ˜¾ç¤ºAnimateæ¨¡å‹
          - **Wan-AI/Wan2.2-Animate-14B**ï¼šé«˜è´¨é‡Animateæ¨¡å‹ï¼Œ14Bå‚æ•°ï¼Œç”¨äºåŸºäºå‚è€ƒå›¾ç‰‡å’Œæ¨¡æ¿è§†é¢‘ç”ŸæˆåŠ¨ç”»
        
        **è¾“å…¥æ¨¡å¼è¯¦ç»†è¯´æ˜**ï¼š
        - **VACEæ¨¡å¼**ï¼šä¸Šä¼ æ·±åº¦è§†é¢‘å’Œ/æˆ–å‚è€ƒå›¾ç‰‡
          - å¯ä»¥å•ç‹¬ä½¿ç”¨æ·±åº¦è§†é¢‘æˆ–å‚è€ƒå›¾ç‰‡
          - ä¹Ÿå¯ä»¥åŒæ—¶ä½¿ç”¨ä¸¤è€…è·å¾—æ›´å¥½çš„æ•ˆæœ
          - æ·±åº¦è§†é¢‘æä¾›è¿åŠ¨ä¿¡æ¯ï¼Œå‚è€ƒå›¾ç‰‡æä¾›è§†è§‰é£æ ¼
        - **é¦–å°¾å¸§æ¨¡å¼**ï¼šä¸Šä¼ é¦–å¸§å›¾ç‰‡ï¼ˆå¿…éœ€ï¼‰å’Œå°¾å¸§å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
          - é¦–å¸§å›¾ç‰‡æ˜¯å¿…éœ€çš„ï¼Œç”¨äºå®šä¹‰è§†é¢‘çš„èµ·å§‹çŠ¶æ€
          - å°¾å¸§å›¾ç‰‡æ˜¯å¯é€‰çš„ï¼Œå¦‚æœæä¾›ä¼šç”Ÿæˆä»é¦–å¸§åˆ°å°¾å¸§çš„è¿‡æ¸¡è§†é¢‘
          - å¦‚æœä¸æä¾›å°¾å¸§ï¼Œåˆ™åªä½¿ç”¨é¦–å¸§ç”Ÿæˆè§†é¢‘
        - **Animateæ¨¡å¼**ï¼šä¸Šä¼ å‚è€ƒå›¾ç‰‡ï¼ˆå¿…éœ€ï¼‰å’Œæ¨¡æ¿è§†é¢‘ï¼ˆå¿…éœ€ï¼‰
          - å‚è€ƒå›¾ç‰‡æ˜¯å¿…éœ€çš„ï¼Œç”¨äºå®šä¹‰è¦åŠ¨ç”»åŒ–çš„ä¸»ä½“å¤–è§‚
          - æ¨¡æ¿è§†é¢‘æ˜¯å¿…éœ€çš„ï¼Œç”¨äºæä¾›åŠ¨ç”»çš„è¿åŠ¨æ¨¡å¼å’Œæ—¶åº
          - ç³»ç»Ÿä¼šå…ˆå°†æ¨¡æ¿è§†é¢‘é¢„å¤„ç†æˆposeå’Œfaceè§†é¢‘ï¼Œç„¶åå°†å‚è€ƒå›¾ç‰‡çš„å¤–è§‚åº”ç”¨åˆ°é¢„å¤„ç†åçš„è§†é¢‘ä¸Š
          - é¢„å¤„ç†è¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
        
        **æ™ºèƒ½æ¨¡å‹åˆ‡æ¢**ï¼š
        - åˆ‡æ¢æ ‡ç­¾é¡µæ—¶ï¼Œæ¨¡å‹é€‰æ‹©ä¼šè‡ªåŠ¨æ›´æ–°ä¸ºå¯¹åº”æ¨¡å¼çš„æ¨¡å‹
        - VACEæ¨¡å¼æ ‡ç­¾é¡µåªæ˜¾ç¤ºVACEæ¨¡å‹ï¼ˆWan-AI/Wan2.1-VACE-*ï¼‰
        - é¦–å°¾å¸§æ¨¡å¼æ ‡ç­¾é¡µåªæ˜¾ç¤ºInPæ¨¡å‹ï¼ˆPAI/Wan2.1-Fun-V1.1-*-InPï¼‰
        - Animateæ¨¡å¼æ ‡ç­¾é¡µåªæ˜¾ç¤ºAnimateæ¨¡å‹ï¼ˆWan-AI/Wan2.2-Animate-14Bï¼‰
        - ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©æ¯ä¸ªæ¨¡å¼çš„é»˜è®¤æ¨¡å‹ï¼ˆ14Bç‰ˆæœ¬ï¼‰
        
        **è§†é¢‘å°ºå¯¸è®¾ç½®**ï¼š
        - **é¢„è®¾å®½é«˜æ¯”æ ‡ç­¾é¡µ**ï¼šé€‰æ‹©é¢„è®¾çš„å®½é«˜æ¯”ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è®¡ç®—å¯¹åº”çš„å°ºå¯¸
          - æ”¯æŒå¤šç§å¸¸ç”¨å®½é«˜æ¯”ï¼š1:1ã€4:3ã€16:9ã€9:16ç­‰
          - åœ¨Dropdownçš„infoä¸­å®æ—¶æ˜¾ç¤ºå½“å‰é€‰æ‹©çš„å°ºå¯¸ï¼ˆå¦‚"å½“å‰å°ºå¯¸: 832 Ã— 480"ï¼‰
          - é€‰æ‹©å®½é«˜æ¯”åä¼šè‡ªåŠ¨æ›´æ–°æ‰‹åŠ¨è®¾ç½®ä¸­çš„æ•°å€¼
        - **æ‰‹åŠ¨è®¾ç½®æ ‡ç­¾é¡µ**ï¼šç›´æ¥è¾“å…¥å®½åº¦å’Œé«˜åº¦æ•°å€¼
          - é«˜åº¦å’Œå®½åº¦èŒƒå›´ï¼š256-1280åƒç´ 
          - å»ºè®®ä½¿ç”¨64çš„å€æ•°ä»¥è·å¾—æœ€ä½³æ€§èƒ½
          - é»˜è®¤å°ºå¯¸ï¼š832x480ï¼ˆ16:9_lowï¼Œé€‚åˆå¤§å¤šæ•°æ˜¾ç¤ºå™¨ï¼‰
        - ä¸¤ä¸ªæ ‡ç­¾é¡µçš„å‚æ•°æ˜¯äº’æ–¥çš„ï¼Œé€‰æ‹©é¢„è®¾å®½é«˜æ¯”æ—¶ä¼šè‡ªåŠ¨æ›´æ–°æ‰‹åŠ¨è®¾ç½®çš„å€¼
        
        **æ³¨æ„äº‹é¡¹**ï¼š
        - VACEæ¨¡å¼ï¼šè‡³å°‘éœ€è¦ä¸Šä¼ æ·±åº¦è§†é¢‘æˆ–å‚è€ƒå›¾ç‰‡ä¸­çš„ä¸€ç§
        - é¦–å°¾å¸§æ¨¡å¼ï¼šå¿…é¡»ä¸Šä¼ é¦–å¸§å›¾ç‰‡ï¼Œå°¾å¸§å›¾ç‰‡å¯é€‰
        - Animateæ¨¡å¼ï¼šå¿…é¡»ä¸Šä¼ å‚è€ƒå›¾ç‰‡å’Œæ¨¡æ¿è§†é¢‘
        - é¦–æ¬¡ç”Ÿæˆæ—¶ä¼šè‡ªåŠ¨åˆå§‹åŒ–æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…
        - Animateæ¨¡å¼çš„é¢„å¤„ç†è¿‡ç¨‹éœ€è¦é¢å¤–æ—¶é—´ï¼ˆé€šå¸¸1-3åˆ†é’Ÿï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…
        - è§†é¢‘ç”Ÿæˆéœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
        - å»ºè®®ä½¿ç”¨è¾ƒå°çš„è§†é¢‘æ–‡ä»¶ä»¥æé«˜å¤„ç†é€Ÿåº¦
        - è¾ƒå¤§çš„è§†é¢‘å°ºå¯¸ä¼šå¢åŠ å¤„ç†æ—¶é—´å’Œæ˜¾å­˜éœ€æ±‚
        - æ·±åº¦è§†é¢‘å°ºå¯¸åº”ä¸ç›®æ ‡å°ºå¯¸å…¼å®¹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒæ•´
        - æ¨¡æ¿è§†é¢‘å°ºå¯¸åº”ä¸ç›®æ ‡å°ºå¯¸å…¼å®¹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒæ•´
        - å¦‚æœå‡ºç°å°ºå¯¸é”™è¯¯ï¼Œè¯·å°è¯•ä½¿ç”¨ä¸åŸå§‹è§†é¢‘ç›¸è¿‘çš„å®½é«˜æ¯”
        - å¦‚æœå‡ºç°VAEè§£ç é”™è¯¯ï¼Œè¯·å°è¯•ç¦ç”¨"Tiled VAE Decode"é€‰é¡¹
        - Animateæ¨¡å¼éœ€è¦é¢„å¤„ç†æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿modelsç›®å½•ä¸‹æœ‰ç›¸åº”çš„æ¨¡å‹æ–‡ä»¶
        
        **é«˜çº§å‚æ•°è¯´æ˜**ï¼š
        - **è§†é¢‘å¸§æ•°**ï¼šæ§åˆ¶ç”Ÿæˆè§†é¢‘çš„é•¿åº¦ï¼Œå¸§æ•°è¶Šå¤šè§†é¢‘è¶Šé•¿
        - **æ¨ç†æ­¥æ•°**ï¼šæ§åˆ¶ç”Ÿæˆè´¨é‡ï¼Œæ­¥æ•°è¶Šå¤šè´¨é‡è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢
        - **æ˜¾å­˜å ç”¨é‡é™åˆ¶**ï¼šæ§åˆ¶æ˜¾å­˜ä½¿ç”¨ï¼Œæ•°å€¼è¶Šå¤§æ˜¾å­˜å ç”¨è¶Šå¤šä½†æ€§èƒ½è¶Šå¥½ï¼ˆ0-100GBï¼‰
        - **Tiled VAE Decode**ï¼šå¯ç”¨åˆ†å—VAEè§£ç ï¼Œå¯æé«˜æ€§èƒ½ä½†å¯èƒ½å¯¼è‡´VAEé”™è¯¯
        
        **è§†é¢‘ä¿å­˜åŠŸèƒ½**ï¼š
        - **ä¿å­˜åœ°å€è®¾ç½®**ï¼šåœ¨"è§†é¢‘ä¿å­˜è®¾ç½®"ä¸­æŒ‡å®šä¿å­˜æ–‡ä»¶å¤¹
        - **æ”¯æŒè·¯å¾„ç±»å‹**ï¼šç›¸å¯¹è·¯å¾„ï¼ˆå¦‚"./outputs"ï¼‰å’Œç»å¯¹è·¯å¾„ï¼ˆå¦‚"/home/user/videos"ï¼‰
        - **è‡ªåŠ¨å­æ–‡ä»¶å¤¹**ï¼šæ¯æ¬¡ç”Ÿæˆä¼šåˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å­æ–‡ä»¶å¤¹ï¼ˆå¦‚"generation_20241201_143022"ï¼‰
        - **æ–‡ä»¶ä¿å­˜**ï¼šè‡ªåŠ¨ä¿å­˜ç”Ÿæˆè§†é¢‘å’Œæ‰€æœ‰è¾“å…¥æ–‡ä»¶ï¼ˆå›¾ç‰‡ã€è§†é¢‘ï¼‰
        - **å‚æ•°è®°å½•**ï¼šç”ŸæˆJSONæ–‡ä»¶è®°å½•æ‰€æœ‰ç”Ÿæˆå‚æ•°ï¼Œä¾¿äºå¤ç°å’Œè°ƒè¯•
        - **æ–‡ä»¶å¤¹ç»“æ„**ï¼š
          ```
          ä¿å­˜æ–‡ä»¶å¤¹/
          â””â”€â”€ generation_20241201_143022/
              â”œâ”€â”€ output_video_12345_1701234567.mp4  # ç”Ÿæˆçš„è§†é¢‘
              â”œâ”€â”€ reference_image.jpg                # å‚è€ƒå›¾ç‰‡
              â”œâ”€â”€ template_video.mp4                 # æ¨¡æ¿è§†é¢‘
              â””â”€â”€ generation_params.json             # ç”Ÿæˆå‚æ•°
          ```
        
        **æ˜¾å­˜ç®¡ç†**ï¼š
        - è°ƒè¯•ä¸­æ–­åç‚¹å‡»"é‡Šæ”¾æ˜¾å­˜"æŒ‰é’®æ¸…ç†æ˜¾å­˜
        - ä½¿ç”¨"åˆ·æ–°æ˜¾å­˜ä¿¡æ¯"æŸ¥çœ‹å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        - æ˜¾å­˜ä¸è¶³æ—¶å»ºè®®å…ˆé‡Šæ”¾æ˜¾å­˜å†é‡æ–°ç”Ÿæˆ
        - å¯ä»¥é€šè¿‡è°ƒæ•´æ˜¾å­˜å ç”¨é‡é™åˆ¶æ¥å¹³è¡¡æ˜¾å­˜ä½¿ç”¨å’Œæ€§èƒ½ï¼ˆ0-100GBæ»‘å—æ§åˆ¶ï¼‰
        - åˆ‡æ¢æ¨¡å‹æ—¶ä¼šè‡ªåŠ¨æ¸…ç†æ˜¾å­˜å¹¶é‡æ–°åˆå§‹åŒ–
        
        **ä½¿ç”¨æç¤º**ï¼š
        - æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ ‡ç­¾é¡µ
        - VACEæ¨¡å¼æ ‡ç­¾é¡µé€‚åˆæœ‰æ·±åº¦è§†é¢‘æˆ–å‚è€ƒå›¾ç‰‡çš„åœºæ™¯
        - é¦–å°¾å¸§æ¨¡å¼æ ‡ç­¾é¡µé€‚åˆæœ‰èµ·å§‹å’Œç»“æŸå›¾ç‰‡çš„åœºæ™¯
        - Animateæ¨¡å¼æ ‡ç­¾é¡µé€‚åˆæœ‰å‚è€ƒå›¾ç‰‡å’Œæ¨¡æ¿è§†é¢‘çš„åœºæ™¯ï¼Œå¯ä»¥ç”ŸæˆåŸºäºæ¨¡æ¿è¿åŠ¨çš„åŠ¨ç”»
        - ç³»ç»Ÿä¼šæ ¹æ®æ ‡ç­¾é¡µè‡ªåŠ¨æ˜¾ç¤ºç›¸åº”çš„è¾“å…¥ç•Œé¢å’Œæ¨¡å‹é€‰é¡¹
        - åˆ‡æ¢æ ‡ç­¾é¡µæ—¶ä¼šè‡ªåŠ¨æ›´æ–°æ¨¡å‹é€‰æ‹©ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´
        """)
    
    return demo

if __name__ == "__main__":
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7861,
        show_error=True
    )
